\chapter{并发模式}

最后一章专门介绍一套用于并发程序的模式。并发和C++有着某种复杂的关系。一方面，C++是一种性能导向的语言，而并发几乎总是用来提升性能，所以两者自然契合。当然，从语言的早期开始，C++就被用来开发并发程序。另一方面，对于一种如此经常用于编写并发程序的语言，C++令人惊讶地缺乏直接解决并发编程需求的构造和特性。这些需求主要通过大量社区开发的库来解决，通常还有特定应用的解决方案。在本章中，我们将回顾并发程序开发中遇到的常见问题以及多年经验中涌现的解决方案；它们共同构成了设计模式的两个方面。

本章涵盖的主题如下：

\begin{itemize}
\item
  C++中并发支持的现状如何？
\item
  并发的主要挑战是什么？
\item
  数据同步的挑战以及满足这些挑战的C++工具
\item
  什么是并发设计？
\item
  在C++中管理并发工作负载的常见模式是什么？
\end{itemize}

\section{技术要求}

本章的示例代码可以在GitHub的以下链接中找到：https://github.com/PacktPublishing/Hands-On-Design-Patterns-with-CPP-Second-Edition/tree/master/Chapter18。此外，并发的基础知识以及C++中并发支持的知识是先决条件。

\section{C++和并发}

并发的概念在C++11中被引入到语言中，但在此之前很久就用C++编写并发程序了。本章不是要介绍并发或者甚至是C++中的并发。这个主题在文献中有很好的覆盖（在本书出版时，Anthony Williams的\emph{C++ Concurrency in Action}是既通用又最新的作品之一）。此外，虽然并发几乎总是用来提升性能，但我们不会在这里直接讨论性能和优化问题；对于这些，你可以参考我的书\emph{The Art of Writing Efficient Programs}。我们将专注于并发软件设计中出现的问题。

广泛地说，在开发并发程序时我们遇到三种类型的挑战。首先，当多个线程并发操作相同数据时，如何确保程序是正确的？其次，如何在多个线程上执行程序的工作以提升整体性能？最后，如何设计软件使我们能够推理它、理解其功能并维护它，这一切都要在并发的额外复杂性下进行。

第一组挑战广泛涉及数据共享和同步。我们将首先检查相关的模式：程序必须首先是正确的，崩溃或产生不可信结果的程序再好的性能也是无用的。

\section{同步模式}

同步模式有一个总体目的：确保对多个线程共享的数据进行正确操作。这些模式对绝大多数并发程序都至关重要。唯一不需要任何同步的程序是那些执行几个完全独立任务的程序，这些任务不涉及任何公共数据（除了可能读取共享且不可变的输入）并产生单独结果。对于其他所有程序，都需要管理一些共享状态，这使我们面临可怕的数据竞争危险。正式地说，C++标准表示，对同一对象（同一内存位置）的并发访问，如果没有适当的同步来保证每个线程的独占访问，就会导致未定义行为。准确地说，如果至少有一个线程可以修改共享数据，行为就是未定义的：如果数据从不被任何线程修改，那么就不可能发生数据竞争。有些设计模式利用了这个漏洞，但让我们从最广为人知的同步模式开始。当你听到避免数据竞争时，首先想到的是什么？

\subsection{互斥锁和锁定模式}

如果说编写并发程序有一个工具的话，那就是互斥锁。互斥锁用于保证对多个线程访问的共享数据的独占访问：

\begin{code}
std::mutex m;
MyData data;
...
// On several threads:
m.lock();
transmogrify(data);
m.unlock();
\end{code}

数据修改操作\texttt{transmogrify()}必须保证对共享数据的独占访问：在任何给定时间只有一个线程可以执行此操作。程序员使用\textbf{互斥锁}（\textbf{mutual exclusion}的缩写）来确保这一点：一次只有一个线程可以锁定互斥锁并进入临界区（\texttt{lock()}和\texttt{unlock()}之间的代码）。

使用互斥锁足以确保对共享数据的正确访问，但这很难说是一个好的设计。第一个问题是它容易出错：如果\texttt{transmogrify()}抛出异常，或者程序员添加了对返回值的检查并提前退出临界区，最终的\texttt{unlock()}永远不会被执行，互斥锁会永远保持锁定状态，从而阻止其他每个线程访问数据。

这个挑战可以通过我们在\emph{第5章，全面了解RAII}中已经看到的非常通用的C++模式的特定应用来轻松解决。我们所需要的只是一个锁定和解锁互斥锁的对象，C++标准库已经提供了一个，\texttt{std::lock\_guard}：

\begin{code}
// Example 01
std::mutex m;
int i = 0;
void add() {
  std::lock_guard<std::mutex> l(m);
  ++i;
}
...
std::thread t1(add);
std::thread t2(add);
t1.join();
t2.join();
std::cout << i << std::endl;
\end{code}

函数\texttt{add()}修改共享变量\texttt{i}，因此需要独占访问；这通过使用互斥锁\texttt{m}来提供。注意，如果你不使用互斥锁运行这个例子，你可能仍然会得到正确的结果，因为其中一个线程会在另一个线程之前执行。有时程序会失败，更多时候不会。这并不使它正确，只是使它难以调试。你可以借助\textbf{线程清理器}（\textbf{TSAN}）看到竞争条件。如果你使用GCC或Clang，添加\texttt{-\/-sanitize=address}来启用它。从\texttt{add()}中删除互斥锁（\emph{示例02}），用TSAN编译，运行程序，你会看到：

\begin{code}
WARNING: ThreadSanitizer: data race
...
Location is global 'i' of size 4 at <address>
\end{code}

还会显示更多信息来帮助你找出哪些线程有数据竞争以及针对哪个变量。这是测试数据竞争比等待程序失败更可靠的方法。

在C++17中，\texttt{std::lock\_guard}的使用稍微简单一些，因为编译器可以从构造函数推断模板参数：

\begin{code}
// Example 03
std::lock_guard l(m);
\end{code}

在C++20中，我们可以使用\texttt{std::jthread}而不是显式调用\texttt{join()}：

\begin{code}
// Example 03
{
  std::jthread t1(add);
  std::jthread t2(add);
}
std::cout << i << std::endl;
\end{code}

注意必须小心在使用计算结果之前销毁线程，因为析构函数现在会join线程并等待计算完成。否则，会有另一个数据竞争：主线程在递增\texttt{i}时读取\texttt{i}的值（TSAN也会发现这个竞争）。

RAII的使用确保每次互斥锁被锁定时也会被解锁，但这并不能避免使用互斥锁时可能发生的其他错误。最常见的是首先忘记使用互斥锁。同步保证只有在每个线程都使用相同的机制来确保对数据的独占访问时才适用。如果即使只有一个线程不使用互斥锁，即使它只是为了读取数据，那么整个程序就是不正确的。

为了防止对共享数据的非同步访问，开发了一种模式。这种模式通常被称为"互斥锁保护"或"互斥锁保护"，它有两个关键要素：首先，需要保护的数据和用于保护的互斥锁被组合在同一个对象中。其次，设计确保对数据的每次访问都受到互斥锁的保护。这里是基本的互斥锁保护类模板：

\begin{code}
// Example 04
template <typename T> class MutexGuarded {
  std::mutex m_;
  T data_ {};
  public:
  MutexGuarded() = default;
  template <typename... Args>
  explicit MutexGuarded(Args&&... args) :
    data_(std::forward<Args>(args)...) {}
  template <typename F> decltype(auto) operator()(F f) {
    std::lock_guard<std::mutex> l(m_);
    return f(data_);
  }
};
\end{code}

正如你所看到的，这个模板结合了互斥锁和被它保护的数据，并只提供一种访问数据的方式：通过调用带有任意可调用对象的\texttt{MutexGuarded}对象。这确保所有数据访问都是同步的：

\begin{code}
// Example 04
MutexGuarded<int> i_guarded(0);
void add() {
  i_guarded([](int& i) { ++i; });
}
...
// On many threads:
std::thread t1(add);
std::thread t2(add);
t1.join();
t2.join();
i_guarded([](int i) { std::cout << i << std::endl; });
\end{code}

这些是正确可靠使用互斥锁的模式的最基本版本。在实践中，需求往往更复杂，解决方案也是如此：有比\texttt{std::mutex}更高效的锁（例如，用于保护短计算的自旋锁，你可以在我的书\emph{The Art of Writing Efficient Programs}中找到），还有更复杂的锁，如用于高效读写访问的共享和独占锁。此外，我们经常必须同时操作几个共享对象，这导致安全锁定多个互斥锁的问题。许多这些问题通过我们刚才看到的模式的更复杂变体来解决。有些需要完全不同的数据访问同步方法，我们将在本节后面看到其中几种。最后，一些数据访问挑战在整体系统设计的更高层次上得到更好的解决；这也将在本章中说明。

让我们接下来回顾除了常用的互斥锁之外的不同数据共享方法。

\subsection{不共享是最好的共享}

虽然用互斥锁保护共享数据看起来并不复杂，但实际上，数据竞争是任何并发程序中最常见的错误。虽然说你不能在不共享的数据上发生数据竞争可能看起来是无用的真理，但不共享是一个经常被忽视的共享替代方案。换句话说，通常可以重新设计程序以避免共享某些变量或将对共享数据的访问限制在代码的较小部分。

这个想法是一个设计模式的基础，它很容易解释但往往难以应用，因为它需要跳出框框思考——线程特定数据模式。它也被称为"线程本地数据"，但这个名称会与C++的\texttt{thread\_local}关键字产生混淆。为了说明这个想法，我们考虑这个例子：我们需要计算可能在多个线程中同时发生的某些事件（对于这个演示，计算什么并不重要）。我们需要整个程序中这些事件的总计数，所以直接的方法是有一个共享计数并在线程检测到事件时递增它（在演示中，我们计算能被10整除的随机数）：

\begin{code}
// Example 05
MutexGuarded<size_t> count;
void events(unsigned int s) {
  for (size_t i = 1; i != 100; ++i) {
    if ((rand_r(&s) % 10) == 0) { // Event!
      count([](size_t& i) { ++i; });
    }
  }
}
\end{code}

这是一个直接的设计；它不是最好的。注意，虽然每个线程都在计算事件，但它不需要知道其他线程计算了多少事件。这不要与事实混淆，即在我们的实现中，每个线程需要知道计数的当前值以便正确地递增它。这种区别是微妙但重要的，并建议一个替代方案：每个线程可以使用线程特定的计数来计算自己的事件，每个线程一个。这些计数都不是正确的，但这没关系，只要我们在需要正确的总事件计数时可以将所有计数加在一起。这里有几种可能的设计。我们可以使用本地计数来计算事件，并在线程退出之前更新一次共享计数：

\begin{code}
// Example 06
MutexGuarded<size_t> count;
void events(unsigned int s) {
  size_t n = 0;
  for (size_t i = 1; i != 100; ++i) {
    if ((rand_r(&s) % 10) == 0) { // Event!
      ++n;
    }
  }
  if (n > 0) count([n](size_t& i) { i += n; });
}
\end{code}

在被一个或多个线程执行的函数中声明的任何本地（栈分配）变量对每个线程都是特定的：每个线程的栈上都有这个变量的唯一副本，当它们都引用相同的名称\texttt{n}时，每个线程访问自己的变量。

我们也可以给每个线程一个唯一的计数变量来递增，并在所有计数线程完成后在主线程中将它们加在一起：

\begin{code}
// Example 07
void events(unsigned int s, size_t& n) {
  for (size_t i = 1; i != 100; ++i) {
    if ((rand_r(&s) % 10) == 0) ++n;
  }
}
\end{code}

在多个线程上调用这个计数函数时，我们必须采取一些预防措施。显然，我们应该给每个线程自己的计数变量\texttt{n}。这还不够：由于被称为"伪共享"的硬件相关效应，我们还必须确保线程特定的计数在内存中不相邻（伪共享的详细描述可以在我的书\emph{The Art of Writing} \emph{Efficient Programs}中找到）：

\begin{code}
// Example 07
alignas(64) size_t n1 = 0;
alignas(64) size_t n2 = 0;
std::thread t1(events, 1, std::ref(n1));
std::thread t2(events, 2, std::ref(n2));
t1.join();
t2.join();
size_t count = n1 + n2;
\end{code}

\texttt{alignas}属性确保每个计数变量的64字节对齐，从而确保\texttt{n1}和\texttt{n2}的地址之间至少有64字节的差异（64是大多数现代CPU（包括X86和ARM）上的缓存行大小）。注意\texttt{std::ref}包装器，这是\texttt{std::thread}调用使用引用参数的函数所需要的。

前面的例子将对共享数据访问的需求减少到每个线程一次，而最后一个例子根本没有任何共享数据；首选的解决方案取决于何时需要总计数的值。

最后一个例子可以从稍微不同的角度来检查；稍微重写它会有帮助：

\begin{code}
// Example 08
struct {
  alignas(64) size_t n1 = 0;
  alignas(64) size_t n2 = 0;
} counts;
std::thread t1(events, 1, std::ref(counts.n1));
std::thread t2(events, 2, std::ref(counts.n2));
t1.join();
t2.join();
size_t count = counts.n1 + counts.n2;
\end{code}

这不会改变任何实质性的东西，但我们可以将线程特定的计数视为同一数据结构的一部分，而不是为每个线程创建的独立变量。这种思维方式引导我们到线程特定数据模式的另一个变体：有时，多个线程必须操作相同的数据，但可能可以分割数据并给每个线程自己的子集来处理。

在下一个例子中，我们需要钳制向量中的每个元素（如果元素超过最大值，它被这个值替换，所以结果总是在零和最大值之间的范围内）。计算由这个模板化算法实现：

\begin{code}
// Example 09
template <typename IT, typename T>
void clamp(IT from, IT to, T value) {
  for (IT it = from; it != to; ++it) {
    if (*it > value) *it = value;
  }
}
\end{code}

生产质量的实现会确保迭代器参数满足迭代器要求，最大值与迭代器值类型可比较，但为了简洁我们省略了所有这些（我们有整整一章关于概念和其他限制模板的方法）。

\texttt{clamp()}函数可以在任何序列上调用，有时我们会幸运地拥有可以在多个线程上独立处理的单独的不相关数据结构。但为了继续这个例子，让我们说我们只有一个需要钳制的向量。然而，一切都没有丢失，因为我们可以在多个线程上处理它的非重叠部分，没有数据竞争的风险：

\begin{code}
// Example 09
std::vector<int> data = ... data ...;
std::thread t1([&](){
  clamp(data.begin(), data.begin() + data.size()/2, 42);
});
std::thread t2([&](){
  clamp(data.begin() + data.size()/2, data.end(), 42);
});
...
t1.join();
t2.join();
\end{code}

即使我们程序中的数据结构在两个线程之间共享并且两个线程都修改它，这个程序是正确的：对于每个向量元素，只有一个线程可以修改它。但是向量对象本身呢？它不是在所有线程之间共享的吗？

我们已经强调过，有一种情况允许数据共享而不需要任何同步：任意数量的线程可以读取相同的变量，只要没有其他线程修改它。我们的例子利用了这一点：所有线程都读取向量的大小和向量对象的其他数据成员，但没有线程改变它们。

线程特定数据模式的应用必须仔细考虑，通常需要对数据结构有很好的理解。我们必须绝对确定没有线程试图修改它们共享的变量，例如向量对象本身的成员的大小和数据指针。例如，如果其中一个线程可以调整向量的大小，那即使没有两个线程访问相同的元素，那也是数据竞争：向量的大小是一个被一个或多个线程修改而没有锁的变量。

我们想在这个小节中描述的最后一个模式适用于几个线程需要修改整个数据集（所以它不能被分割）但线程不需要看到其他线程所做的修改的情况。通常，这发生在修改是作为某些结果计算的一部分完成的，但修改的数据本身不是最终结果。在这种情况下，有时最好的方法是为每个线程创建数据的线程特定副本。当这样的副本是一个"一次性"对象时，这种模式工作得最好：每个线程需要修改其副本，但修改的结果不需要提交回原始数据结构。

在下面的例子中，我们使用一个算法来计算向量中的唯一元素，该算法就地排序向量：

\begin{code}
// Example 10
void count_unique(std::vector<int> data, size_t& count) {
  std::sort(data.begin(), data.end());
  count = std::unique(data.begin(),
                      data.end()) - data.begin();
}
\end{code}

此外，当我们需要只计算满足谓词的元素时，我们首先擦除所有其他元素（\texttt{std::erase\_if}是C++20的添加，但在先前版本的C++中很容易实现）：

\begin{code}
// Example 10
void count_unique_even(std::vector<int> data, size_t& count) {
  std::erase_if(data, [](int i) { return i & 1; });
  std::sort(data.begin(), data.end());
  count = std::unique(data.begin(),
                      data.end()) - data.begin();
}
\end{code}

两者都是对向量的破坏性操作，但它们只是达到目的的手段：一旦我们有了计数，改变的向量就可以被丢弃。同时在几个线程上计算我们的计数的最简单、通常也是最高效的方法是制作向量的线程特定副本。实际上，我们已经这样做了：两个计数函数都通过值接受向量参数，因此制作副本。通常，这会是一个错误，但在我们的情况下，这是有意的，允许两个函数同时在同一个向量上操作：

\begin{code}
// Example 10
std::vector<int> data = ...;
size_t unique_count = 0;
size_t unique_even_count = 0;
{
  std::jthread t1(count_unique, data,
                  std::ref(unique_count));
  std::jthread t2(count_unique_even, data,
                  std::ref(unique_even_count));
}
\end{code}

当然，对原始数据仍然有并发访问，并且这是在没有锁的情况下完成的：两个线程都需要制作它们的线程特定副本。然而，这属于只读并发访问的例外，是安全的。

原则上，在可能的情况下避免数据共享，否则使用互斥锁，足以在任何程序中安排无竞争的数据访问。然而，这可能不是实现这个目标的有效方式，而良好的性能几乎总是并发的目标。我们现在将考虑其他几种用于并发访问共享数据的模式，在适用时，可以提供优越的性能。我们将从超越互斥锁的同步原语开始，这些原语专门设计用于允许线程高效地等待某些事件。

\subsection{等待模式}

等待是并发程序中经常遇到的问题，有多种形式。我们已经看到了一种：互斥锁。确实，如果两个线程试图同时进入临界区，其中一个将不得不等待。但这里等待不是目标，只是临界区独占访问的不幸副作用。还有其他等待是主要目标的情况。例如，我们可能有等待某些事件发生的线程。这可能是等待输入的用户界面线程（很少或没有性能要求）或等待网络套接字的线程（中等性能要求）甚至高性能线程，如线程池中等待执行任务的计算线程（极高性能要求）。毫不奇怪，这些场景有不同的实现，但从根本上有两种方法：轮询和通知。我们将首先看通知。

等待通知的基本模式是\textbf{条件模式}。它通常由条件变量和互斥锁组成。一个或多个线程被阻塞等待条件变量。在此期间，还有一个线程锁定互斥锁（从而保证独占访问）并执行其他线程正在等待完成的工作。一旦工作完成，完成工作的线程必须释放互斥锁（以便其他线程可以访问包含此工作结果的共享数据）并通知等待线程它们可以继续。

例如，在线程池中，等待线程是等待任务被添加到池中的池工作线程。由于池任务队列是共享资源，线程需要独占访问来推送或弹出任务。向队列添加一个或多个任务的线程必须在这样做时持有互斥锁，然后通知工作线程有任务要执行。

让我们现在看一个只有两个线程的通知模式的非常基本的例子。首先，我们有启动工作线程然后等待它产生一些结果的主线程：

\begin{code}
// Example 11
std::mutex m;
std::condition_variable cv;
size_t n = 0;               // Zero until work is done
// Main thread
void main_thread() {
  std::unique_lock l(m);
  std::thread t(produce);     // Start the worker
  cv.wait(l, []{ return n != 0; });
  ... producer thread is done, we have the lock ...
}
\end{code}

在这种情况下的锁定由\texttt{std::unique\_lock}提供，这是一个包装互斥锁并具有类似互斥锁的接口的对象，有\texttt{lock()}和\texttt{unlock()}成员函数。互斥锁在构造函数中被锁定，当我们开始等待条件时，几乎立即被\texttt{wait()}函数解锁。当收到通知时，\texttt{wait()}在返回控制给调用者之前再次锁定互斥锁。

许多等待和条件的实现都受到所谓的虚假唤醒的影响：即使没有通知，等待也可能被中断。这就是为什么我们还要检查结果是否准备好，在我们的情况下，通过检查结果计数\texttt{n}：如果它仍然是零，就没有结果，主线程被错误地唤醒，我们可以回到等待（注意等待线程在\texttt{wait()}返回之前仍然必须获取互斥锁，所以它必须等待工作线程释放这个互斥锁）。

工作线程必须在访问共享数据之前锁定相同的互斥锁，然后在通知主线程工作完成之前解锁它：

\begin{code}
// Example 11
// Worker thread
void produce() {
  {
    std::lock_guard l(m);
    ... compute results ...
    n = ... result count ...
  } // Mutex unlocked
  cv.notify_one();          // Waiting thread notified
}
\end{code}

没有必要在工作线程活动的整个时间内持有互斥锁：它的唯一目的是保护共享数据，如我们例子中的结果计数\texttt{n}。

两个同步原语\texttt{std::conditional\_variable}和\texttt{std::unique\_lock}是用于实现带条件的等待模式的标准C++工具。就像互斥锁一样，有许多变化。

通知的替代方案是轮询。在这种模式中，等待线程重复检查某些条件是否满足。在C++20中，我们可以使用\texttt{std::atomic\_flag}实现一个简单的轮询等待例子，它本质上是一个原子布尔变量（在C++20之前我们可以用\texttt{std::atomic\textless{}bool\textgreater{}}做同样的事情）：

\begin{code}
// Example 12
std::atomic_flag flag;
// Worker thread:
void produce() {
  ... produce the results ...
  flag.test_and_set(std::memory_order_release);
}
// Waiting thread:
void main_thread() {
  flag.clear();
  std::thread t(produce);
  while (!flag.test(std::memory_order_acquire)) {} // Wait
  ... results are ready ...
}
\end{code}

像\texttt{test\_and\_set()}这样的原子操作使用\textbf{内存屏障}：一种全局同步标志，确保在标志被设置（释放）之前对内存的所有更改对在标志被测试（获取）之后在任何其他线程上执行的任何操作都是可见的。这些屏障还有很多内容，但它超出了本书的范围，可以在许多专门讨论并发和效率的书籍中找到。

这个例子和前一个例子之间最重要的区别是\emph{示例12}中等待线程的显式轮询循环。如果等待时间很长，这是非常低效的，因为等待线程在等待的整个时间内都在忙于计算（从内存中读取）。任何实际实现都会在等待循环中引入一些睡眠，但这样做也是有代价的：等待线程不会在工作线程设置标志后立即唤醒，而必须首先完成睡眠。这些效率问题超出了本书的范围；这里我们想展示这些模式的整体结构和组件。

轮询和等待之间的界限并不总是清楚的。例如，据我们所知，\texttt{wait()}可能通过定期轮询条件变量的某些内部状态来实现。事实上，我们刚才看到的相同原子标志可以用来等待通知：

\begin{code}
// Example 13
std::atomic_flag flag;
// Worker thread:
void produce() {
  ... produce the results ...
  flag.test_and_set(std::memory_order_release);
  flag.notify_one();
}
// Waiting thread:
void main_thread() {
  flag.clear();
  std::thread t(produce);
  flag.wait(true, std::memory_order_acquire); // Wait
  while (!flag.test(std::memory_order_acquire)) {}
  ... results are ready ...
}
\end{code}

对\texttt{wait()}的调用需要相应的\texttt{notify\_one()}调用（如果我们有多个线程等待标志，则需要\texttt{notify\_all()}）。它的实现几乎肯定比我们简单的轮询循环更高效。通知接收到且等待结束后，我们检查标志以确保它真的被设置了。标准说这不是必要的，\texttt{std::atomic\_flag::wait()}不会遭受虚假唤醒，但GCC和Clang中的TSAN都不同意（这可能是TSAN中的误报或标准库实现中的错误）。

还有许多其他需要等待的情况，我们需要等待的条件变化很大。另一个常见需求是等待一定数量的事件发生。例如，我们可能有几个产生结果的线程，我们可能需要它们全部完成各自的工作份额，然后主线程才能继续。这通过等待屏障或闩锁来完成。在C++20之前，我们需要自己实现这些同步原语或使用第三方库，但在C++20中它们成为了标准：

\begin{code}
// Example 14
// Worker threads
void produce(std::latch& latch) {
  ... do the work ...
  latch.count_down();     // One more thread is done
}
void main_thread() {
  constexpr size_t nthread = 4;
  std::jthread t[nthread];
  std::latch latch(nthread); // Wait for 4 count_down()
  for (size_t i = 0; i != nthread; ++i) {
    t[i] = std::jthread(std::ref(latch));
  }
  latch.wait();   // Wait for producers to finish
  ... results are ready ...
}
\end{code}

闩锁用要等待的事件计数初始化。当已经完成那么多\texttt{count\_down()}调用时，它将解锁。

等待还有许多其他应用，但几乎所有等待模式都广泛地归入我们在本节中看到的类别之一（特定实现可能对特定情况下的性能产生戏剧性影响，这就是为什么你更可能看到这些同步构造的自定义应用特定版本，而不是找到非标准容器或其他基本数据结构）。

我们现在将看到几个非常专门和非常高效的同步模式的例子。它们不适用于所有情况，但当它们符合需要时，它们通常提供最好的性能。

\subsection{无锁同步模式}

大多数时候，安全访问共享数据依赖于互斥锁。C++还支持另一种同步并发线程的类型：原子操作。同样，详细的解释超出了本书的范围，本节需要一些关于原子的先验知识。

基本思想是这样的：一些数据类型（通常是整数）有特殊的硬件指令，允许少数简单操作（如读取或写入或递增值）原子地、在单个事件中完成。在这个原子操作期间，其他线程根本无法访问原子变量，所以如果一个线程执行原子操作，所有其他线程可以看到相同的变量，因为它在操作之前或操作之后，但不是在操作中间。例如，递增是一个读-修改-写操作，但原子递增是一个特殊的硬件事务，使得一旦读取开始，其他线程就不能访问变量，直到写入完成。这些原子操作通常伴随着内存屏障；我们已经使用它们来确保不仅原子的而且程序中所有变量上的所有其他操作都是同步的，没有数据竞争。

原子操作的最简单但有用的应用是计数。我们经常需要在程序中计算某些东西，在并发程序中，我们可能需要计算可能在多个线程上发生的某些事件。如果我们只对所有线程完成后的总计数感兴趣，这最好通过我们早期看到的"非共享"或线程特定计数器来处理。但是如果所有线程也需要知道当前计数呢？我们总是可以使用互斥锁，但使用互斥锁来保护整数的简单递增是非常低效的。C++给了我们一个更好的方法，原子计数器：

\begin{code}
// Example 15
std::atomic<size_t> count;
void thread_work() {
  size_t current_count = 0;
  if (... counted even ...) {
    current_count =
      count.fetch_add(1, std::memory_order_relaxed);
  }
}
\end{code}

在这个例子中只有一个共享变量，\texttt{count}本身。由于我们没有任何其他共享数据，我们不需要内存屏障（"宽松"内存顺序意味着对其他数据的访问顺序没有要求）。\texttt{fetch\_add()}操作是原子递增，它将\texttt{count}递增1并返回\texttt{count}的旧值。

原子计数也可以用来让多个线程在相同的数据结构上工作而不需要任何锁定：为了做到这一点，我们需要确保只有一个线程在数据结构的每个元素上工作。当以这种方式使用时，该模式通常被称为原子索引。在下一个例子中，我们有一个在所有线程之间共享的数据数组：

\begin{code}
// Example 16
static constexpr size_t N = 1024;
struct Data { ... };
Data data[N] {};
\end{code}

我们还有一个原子索引，它被所有需要在数组中存储其工作结果的线程使用。为了安全地做到这一点，每个线程递增原子索引并使用前递增值作为数组的索引。没有两个原子递增操作可以产生相同的值，因此，每个线程获得自己的数组元素来处理：

\begin{code}
// Example 16
std::atomic<size_t> index(0);
// Many producer threads
void produce(size_t& n) {
  while (... more work 鈥?) {
    const size_t s =
      index.fetch_add(1, std::memory_order_relaxed);
    if (s >= N) return;     // No more space
    data[s] = ... results ...
  }
}
\end{code}

每个线程可以初始化任意多个数组元素，当它（和所有其他线程）填满整个数组时停止。主线程必须等到所有工作完成后才能访问任何结果。这不能仅仅用原子索引来完成，因为当线程开始处理特定数组元素时索引被递增，而不是当该线程完成工作时。我们必须使用其他一些同步机制来使主线程等待直到所有工作完成，例如闩锁或在简单情况下，连接生产者线程：

\begin{code}
// Example 16
void main_thread() {
  constexpr size_t nthread = 5;
  std::thread t[nthread];
  for (size_t i = 0; i != nthread; ++i) {
    t[i] = std::thread(produce);
  }
  // Wait for producers. to finish.
  for (size_t i = 0; i != nthread; ++i) {
    t[i].join();
  }
  ... all work is done, data is ready ...
}
\end{code}

The atomic count is good when we don't rely on the value of the count to access the results that are already produced. In the last example, the producer threads did not need access to the array elements computed by other threads, and the main thread waits for all threads to complete before accessing the results. Often, this is not the case and we need to access data as it is being produced. This is where memory barriers come in.

The simplest but surprisingly powerful lock-free pattern that relies on memory barriers is known as the publishing protocol. The pattern is applicable when one thread is producing some data that is going to be made accessible to one or more other threads when it is ready. The pattern looks like this:

\begin{code}
// Example 17
std::atomic<Data*> data;
void produce() {
  Data* p = new Data;
  ... complete *p object ...
  data.store(p, std::memory_order_release);
}
void consume() {
  Data* p = nullptr;
  while (!(p = data.load(std::memory_order_acquire))) {}
  ... safe to use *p ...
}
\end{code}

The shared variable is an atomic pointer to the data. It is often called the ``root'' pointer because the data itself may be a complex data structure with multiple pointers connecting its parts. The key requirement of this pattern is that there is only one way to access the entire data structure and this is through the root pointer.

The producer thread builds all the data it needs to produce. It uses a thread-specific pointer, usually a local variable, to access the data. No other thread can see the data yet because the root pointer does not point to it and the local pointer of the producer thread is not shared with other threads.

Finally, when the data is complete, the producer atomically stores the pointer to the data in the shared root pointer. It is often said that the producer atomically publishes the data, hence the name of the pattern, the publishing protocol.

The consumers must wait for the data to be published: as long as the root pointer is null, there is nothing for them to do. They wait for the root pointer to become non-null (the wait does not have to use polling, a notification mechanism is also possible). Once the data is published, the consumer threads can access it through the root pointer. Because there is no other synchronization, no thread can modify the data once it's published (the data may contain a mutex or some other mechanism to allow parts of it to be modified safely).

The atomic variable itself is insufficient for this pattern to guarantee no data races: all threads access not just the atomic pointer but the memory it points to. This is why we needed the specific memory barriers: when publishing the data, the producer uses the release barrier to not only initialize the pointer atomically but also ensure that all memory modifications that were done before the atomic write operations on the pointer become visible to anyone who reads the new value of the pointer. The consumer uses the acquire barrier to ensure that any operation on the shared data that is done after the new value of the pointer is read observes the latest state of the shared data as it existed at the moment the data was published. In other words, if you read the value of the pointer and then dereference it, you generally do not know if you will get the latest value of the data the pointer points to. But if you read the pointer with the acquire barrier (and the pointer was written with the release barrier), then you can be sure that you will read (acquire) the data as it was last written (released). Together, the release and acquire barriers guarantee that the consumer sees the shared data exactly as it was seen by the producer at the moment it published the address of the data in the root pointer.

The same pattern can be used to publish completed elements of a larger data structure shared between threads. For example, we can have a producer thread publish how many array elements it initialized with the results:

\begin{code}
// Example 18
constexpr size_t N = ...;
Data data[N];     // Shared, not locked
std::atomic<size_t> size;
void produce() {
  for (size_t n = 0; n != N; ++n) {
    data[n] = ... results ...
    size.store(n, std::memory_order_release);
  }
}
void consume() {
  size_t n = 0;
  do {
    n = size.load(std::memory_order_acquire);
    ... n elements are safe to access ...
  } while (n < N - 1);
}
\end{code}

The idea is exactly the same as in the previous example, only instead of the pointer we use the index into an array. In both cases, we have one producer thread that computes and publishes data, and one or more consumer threads that wait for the data to be published. If we need multiple producers, we must use some other synchronization mechanism to ensure that they don't work on the same data, such as the atomic index we just saw.

In a program with multiple producer and consumer threads, we often have to combine several synchronization patterns. In the next example, we have a large shared data structure organized as an array of pointers to the individual elements. Several producer threads fill this data structure with results; we are going to use the atomic index to ensure that each element is handled by only one producer:

\begin{code}
// Example 19
static constexpr size_t N = 1024;
struct Data { ... };
std::atomic<Data*> data[N] {};
std::atomic<size_t> size(0);     // Atomic index
void produce() {
  Data* p = new Data;
  ... compute *p ...
  const size_t s =
    size.fetch_add(1, std::memory_order_relaxed);
  data[s].store(p, std::memory_order_release);
}
\end{code}

Our producer computes the result, then fetches the current index value and, at the same time, increments the index so the next producer cannot get the same index value. The array slot \texttt{data{[}s{]}} is, therefore, uniquely reserved for this producer thread. This is enough to avoid sharing conflicts between producers, but the consumers cannot use the same index to know how many elements are already in the array: the index is incremented before the corresponding array element is initialized. For the consumers, we use the publishing protocol: each array element is an atomic pointer that remains null until the data is published. The consumers must wait for a pointer to become non-null before they can access the data:

\begin{code}
// Example 19
void consumer() {
  for (size_t i = 0; i != N; ++i) {
    const Data* p =
      data[i].load(std::memory_order_acquire);
    if (!p) break; // No more data
    ... *p is safe to access ...
  }
}
\end{code}

In this example, the consumer stops as soon as it finds a data element that is not ready. We could continue scanning the array: some of the subsequent elements may be ready because they were filled by another producer thread. If we do, we have to somehow remember to come back to handle the elements we missed. The right approach depends on the problem we need to solve, of course.

The literature on lock-free programming is extensive and full of (usually) very complex examples. The concurrency patterns we have demonstrated are only the basic building blocks for more complex data structures and data synchronization protocols.

In the next section, we will see some of the much higher-level patterns that are applicable to the design of such data structures or even entire programs and their major components.

\section{Concurrent design patterns and guidelines}

Designing and implementing concurrent software is hard. Even the basic patterns for controlling access to shared data, such as the ones we saw in the last section, are complex and full of subtle details. Failing to notice one of these details usually results in hard-to-debug data races. To simplify the task of writing concurrent programs, the programming community came up with several guidelines. All of them arise out of earlier disastrous experiences, so take these guidelines seriously. Central to these guidelines is the concept of thread safety guarantees.

\subsection{Thread safety guarantees}

While this is not a pattern, it is a concept that is much broader in scope and one of the key design principles for any concurrent software. Every class, function, module, or component of a concurrent program should specify the thread safety guarantees it provides, as well as the guarantees it requires from the components it uses.

In general, a software component can offer three levels of thread-safety guarantees:

\begin{itemize}
\item
  \textbf{Strong thread safety guarantee}: Any number of threads can access this component without restrictions and without encountering undefined behavior. For a function, it means that any number of threads can call this function at the same time (possibly, with some restrictions on parameters). For a class, it means that any number of threads can call member functions of this class concurrently. For a larger component, any number of threads can operate its interfaces (again, possibly with some restrictions). Such components, classes, and data structures are sometimes called thread-safe.
\item
  \textbf{Weak thread safety guarantee}: Any number of threads can access this component for operations that are specified to not alter the state of the component (for a class, this is usually \texttt{const} member functions). Only one thread can modify the state of the component at any time, and the locking or another way of ensuring such exclusive access is the responsibility of the caller. Such components, classes, and data structures are sometimes called thread-compatible because you can build a concurrent program from them using the appropriate synchronization mechanisms. All STL containers offer this level of guarantee.
\item
  \textbf{No thread-safety guarantee}: Such components cannot be used in a concurrent program at all and are sometimes called thread-hostile. These classes and functions often have hidden global states that cannot be accessed in a thread-safe manner.
\end{itemize}

By designing each component to provide certain thread safety guarantees, we can divide the intractable problem of making the entire program thread-safe into a hierarchy of design challenges where the more complex components take advantage of the guarantees provided by the simpler ones. Central to this process is the notion of the transactional interface.

\subsection{Transactional interface design}

The idea of the transactional interface design is very simple: every component should have an interface such that every operation is an atomic transaction. From the point of view of the rest of the program, the operation either has not happened yet or is done. No other thread can observe the state of the component during the operation. This can be accomplished using mutexes or any other synchronization scheme that fits the need -- the particular implementation can influence performance but is not essential for correctness as long as the interface guarantees transaction processing.

This guideline is most useful for designing data structures for concurrent programs. Here, it is so important that it is generally accepted that one cannot design a thread-safe data structure that does not offer a transactional interface (at least not a useful data structure). For example, we can consider a queue. The C++ standard library offers a \texttt{std::queue} template. As with any other STL container, it offers the weak guarantee: any number of threads can call \texttt{const} methods of the queue as long as no thread calls any non-\texttt{const} methods. Alternatively, any one thread can call a non-\texttt{const} method. To ensure the latter, we have to lock all accesses to the queue with an external mutex. If we want to pursue this approach, we should combine the queue and the mutex in a new class:

\begin{code}
template <typename T> class ts_queue {
  std::mutex m_;
  std::queue<T> q_;
  public:
  ts_queue() = default;
  ...
};
\end{code}

To push another element onto a queue, we need to lock the mutex, since the \texttt{push()} member function modifies the queue:

\begin{code}
template <typename T> class ts_queue {
  public:
  void push(const T& t) {
    std::lock_guard l(m_);
    q_.push(t);
  }
};
\end{code}

This works exactly as we want it to: any number of threads can call \texttt{push()} and every element will be added to the queue exactly once (the order is going to be arbitrary if multiple calls happen simultaneously, but this is the nature of concurrency). We have successfully provided the strong thread safety guarantee!

The triumph is going to be short-lived, unfortunately. Let us see what it takes to pop an element from the queue. There is a member function \texttt{pop()} that removes the element from the queue, so we can protect it with the same mutex:

\begin{code}
template <typename T> class ts_queue {
  public:
  void pop() {
    std::lock_guard l(m_);
    q_.pop();
  }
};
\end{code}

Notice that this function does not return anything: it removes the oldest element in the queue and destroys it, but that's not what we need to find out what that element is (or was). For that, we need to use the function \texttt{front()} which returns a reference to the oldest element but does not modify the queue. It is a \texttt{const} member function, so we need to lock it only if we call any non-\texttt{const} functions at the same time; we are going to ignore this optimization possibility for now and always lock this call as well:

\begin{code}
template <typename T> class ts_queue {
  public:
  T& front() const {
    std::lock_guard l(m_);
    return q_.front();
  }
};
\end{code}

If we call \texttt{front()} from multiple threads and don't call any other functions, this implementation is sub-optimal, but it is not wrong.

There is one special case we have neglected to mention: if the queue is empty, you should not call \texttt{pop()} or \texttt{front()} -- doing so leads to undefined behavior, according to the standard. How do you know if it is safe to pop an element from the queue? You can check if the queue is empty. This is another \texttt{const} member function, and again we are going to over-protect it and lock every call to it:

\begin{code}
template <typename T> class ts_queue {
  public:
  bool empty() const {
    std::lock_guard l(m_);
    return q_.empty();
  }
};
\end{code}

Now every member function of the underlying \texttt{std::queue} is protected by a mutex. We can call any of them from any number of threads and be guaranteed that only one thread can access the queue at any time. Technically, we have achieved the strong guarantee. Unfortunately, it is not very useful.

To see why, let us consider the process of removing an element from the queue:

\begin{code}
ts_queue<int> q;
int i = 0;
if (!q.empty()) {
  i = q.front();
  q.pop();
}
\end{code}

This works fine on one thread, but we didn't need a mutex for that. It still (mostly) works when we have two threads, one of which is pushing new elements onto the queue and the other one is taking them from the queue. Let us consider what happens when two threads try to pop one element each. First, they both call \texttt{empty()}. Let us assume that the queue is not empty and both calls return \texttt{true}. Then, they both call \texttt{front()}. Since neither thread did a \texttt{pop()} yet, both threads get the same front element. This is not what was supposed to happen if we want each thread to pop an element from the queue. Finally, both threads call \texttt{pop()}, and two elements are removed from the queue. One of these elements we have never seen and will never see again, so we lost some of the data that was enqueued.

But this isn't the only way it can go wrong. What happens if there is only one element on the queue? Both calls to \texttt{empty()} still return true -- a queue with one element is not empty. Both calls to \texttt{front()} still return the (same) front element. The first call to \texttt{pop()} succeeds but the second one is undefined behavior because the queue is now empty. It is also possible that one thread calls \texttt{pop()} before the other thread calls \texttt{front()} but after it calls \texttt{empty()}. In this case, the second call to \texttt{front()} is also undefined.

We have a perfectly safe and a perfectly useless data structure. Clearly, a thread safety guarantee is not enough. We also need an interface that does not expose us to undefined behavior, and the only way to do this is to perform all three steps of the pop operation (\texttt{empty()}, \texttt{front()}, and \texttt{pop()}) in a single critical section, i.e., without releasing the mutex between the calls. Unless we want the caller to supply their own mutex, the only way to do this is to change the interface of our \texttt{ts\_queue} class:

\begin{code}
// Example 20
template <typename T> class ts_queue {
  std::queue<T> q_;
  std::mutex m_;
  public:
  ts_queue() = default;
  template <typename U> void push(U&& u) {
    std::lock_guard l(m_);
    q_.push(std::forward<U>(u));
  }
  std::optional<T> pop() {
    std::lock_guard l(m_);
    if (q_.empty()) return {};
    std::optional<T> res(std::move(q_.front()));
    q_.pop();
    return res;
 }
};
\end{code}

The \texttt{push()} function is the same as it was before (we made the argument type more flexible, but this is not related to thread safety). The reason we did not need to change the push operation is that it is already transactional: at the end, the queue has one more element than it had at the beginning of the operation, and the state of the queue is otherwise identical. We just made it atomic by protecting it with a mutex (no other thread that also uses the same mutex correctly can observe our queue in its transitional non-invariant state).

The \texttt{pop()} operation is where the transactional interface looks very different. In order to provide a meaningful thread safety guarantee, we have to provide an operation that returns the front element to the caller and removes it from the queue atomically: no other thread should be able to see the same front element, therefore, we have to lock both \texttt{front()} and \texttt{pop()} on the original queue with the same mutex. We also have to consider the possibility that the queue is empty and we have no front element to return to the caller. What do we return in this case? If we decided to return the front element by value, we would have to default-construct this value (or return some other agreed-upon value that means ``no element''). In C++17, a better way is to return a \texttt{std::optional} that holds the front element if there is one.

Now both \texttt{pop()} and \texttt{push()} are atomic and transactional: we can call both methods from as many threads as we want, and the results are always well-defined.

You may wonder why didn't \texttt{std::queue} offer this transactional interface, to begin with. First, STL was designed long before threads made it into the standard. But the other, very important, reason is that the queue interface was influenced by the need to provide exception safety. Exception safety is the guarantee that the object remains in a well-defined state if an exception is thrown. Here, the original queue interface does very well: \texttt{empty()} just returns the size and cannot throw an exception, \texttt{front()} returns the reference to the front element and also cannot throw, and finally \texttt{pop()} calls the destructor of the front element, which normally does not throw either. Of course, when accessing the front element, the caller's code may throw (for example, if the caller needs to copy the front element to another object) but the caller is expected to handle that. In any case, the queue itself remains in a well-defined state.

Our thread-safe queue, however, has an exception safety problem: the code that copies the front element of the queue to return it to the caller is now inside \texttt{pop()}. If the copy constructor throws during the construction of the local \texttt{std::optional} variable \texttt{res}, we are probably OK. However, if an exception is thrown when the result is returned to the caller (which can happen by move or copy), then \texttt{pop()} was already done, so we are going to lose the element we just popped from the queue.

This tension between thread safety and exception safety is often unavoidable and has to be considered when designing thread-safe data structures for concurrent programs. Regardless, it must be reiterated that the only way to design thread-safe data structures or larger modules is to ensure that every interface call is a complete transaction: any steps that are conditionally defined must be packaged into a single transactional call together with the operations that are needed to ensure that such conditions are met. Then, the entire call should be guarded by a mutex or some other way to ensure race-free exclusive access.

Designing thread-safe data structures is generally very hard, especially if we want good performance (and what is the point of concurrency if we don't?). That is why it is very important to take advantage of any use restrictions or special requirements that allow us to impose restrictions on how these data structures are used. In the next section, we will see one common case of such restrictions.

\subsection{Data structures with access limitations}

Designing thread-safe data structures is so hard that one should look for any opportunity to simplify the requirements and the implementation. If there is any scenario you don't need right now, think if you can make your code simpler if you do not support that scenario. One obvious case is a data structure that is built by a single thread (no thread safety guarantees needed) then becomes immutable and is accessed by many threads that act as readers (a weak guarantee is sufficient). Any STL container, for example, can operate in this mode as-is. We still need to ensure that no reader can access the container while it's still being filled with data, but that can be easily done with a barrier or a condition. This is a very useful but rather trivial case. Are there any other restrictions we can make use of?

In this section, we consider a particular use case that occurs quite frequently and allows for much simpler data structures. Specifically, we examine the situation when a particular data structure is accessed by only two threads. One thread is the producer, it adds data to the data structure. The other thread is the consumer, it removes the data. Both threads do modify the data structure but in different ways. This situation occurs rather frequently and often allows for very specialized and very efficient data structure implementations. It probably deserves recognition as a design pattern for concurrent designs, and it already has a commonly recognized name: ``single-producer single-consumer data structure.''

In this section, we are going to see an example of a single-producer single-consumer queue. It is a data structure that is frequently used with one producer and one consumer thread, but the ideas we explore here can be used to design other data structures as well. The main distinguishing feature of this queue is going to be that it is lock-free: there are no mutexes in it at all, so we can expect much higher performance from it.

The queue is built on an array of a fixed size, so, unlike a regular queue, it cannot grow indefinitely (this is another common restriction used to simplify lock-free data structures):

\begin{code}
// Example 21
template <typename T, size_t N> class ts_queue {
  T buffer_[N];
  std::atomic<size_t> back_{0};
  std::atomic<size_t> front_{N - 1};
  ...
};
\end{code}

In our example, we default-construct elements in the array. If this is undesirable, we can also use a properly aligned uninitialized buffer. All accesses to the queue are determined by two atomic variables, \texttt{back\_} and \texttt{front\_}. The former is the index of the array element that we will write into when we push a new element onto the queue. The latter is the index of the array element we will read from when we need to pop an element from the queue. All array elements in the range {[}\texttt{front\_}, \texttt{back\_}) are filled with elements currently on the queue. Note that this range can wrap over the end of the buffer: after using the element \texttt{buffer\_{[}N-1{]}} the queue does not run out of space but starts again from \texttt{buffer\_{[}0{]}}. This is known as a \textbf{circular buffer}.

How do we use these indices to manage the queue? Let us start with the push operation:

\begin{code}
// Example 21
template <typename T, size_t N> class ts_queue {
  public:
  template <typename U> bool push(U&& u) {
    const size_t front =
      front_.load(std::memory_order_acquire);
    size_t back = back_.load(std::memory_order_relaxed);
    if (back == front) return false;
    buffer_[back] = std::forward<U>(u);
    back_.store((back + 1) % N, std::memory_order_release);
    return true;
  }
};
\end{code}

We need to read the current value of \texttt{back\_}, of course: this is the index of the array element we are about to write. We support only one producer, and only the producer thread can increment \texttt{back\_}, so we do not need any particular precautions here. We do, however, need to be careful to avoid overwriting any elements already in the queue. To do this we must check the current value of \texttt{front\_} (we can read it before or after reading \texttt{back\_}, it makes no difference). If the element \texttt{buffer\_{[}back{]}} that we are about to overwrite is also the front element, then the queue is full and the \texttt{push()} operation fails (note that there is another solution to this problem that is often used in real-time systems: if the queue is full, the oldest element is silently overwritten and lost). After the new element is stored, we atomically increment the \texttt{back\_} value to signal to the consumer that this slot is now available for reading. Because we are publishing this memory location, we must use the release barrier. Also note the modular arithmetic: after reaching the array element \texttt{N-1}, we're looping back to element 0.

Next, let us see the \texttt{pop()} operation:

\begin{code}
// Example 21
template <typename T, size_t N> class ts_queue {
  public:
  std::optional<T> pop() {
    const size_t back =
      back_.load(std::memory_order_acquire);
    const size_t front =
     (front_.load(std::memory_order_relaxed) + 1) % N;
    if (front == back) return {};
    std::optional<T> res(std::move(buffer_[front]));
    front_.store(front, std::memory_order_release);
    return res;
  }
};
\end{code}

Again, we need to read both \texttt{front\_} and \texttt{back\_}: \texttt{front\_} is the index of the element we are about to read, and only the consumer can advance this index. On the other hand, \texttt{back\_} is needed to make sure we actually have an element to read: if the front and back are the same, the queue is empty; again, we use \texttt{std::optional} to return a value that might not exist. We must use acquire barrier when reading \texttt{back\_} to make sure we see the element values that were written into the array by the producer thread. Finally, we advance \texttt{front\_} to ensure that we don't read the same element again and to make this array slot available to the producer thread.

There are several subtle details here that must be pointed out. Reading \texttt{back\_} and \texttt{front\_} is not done in a single transaction (it is not atomic). In particular, if the producer reads \texttt{front\_} first, it is possible that, by the time it reads \texttt{back\_} and compares the two, the consumer has already advanced \texttt{front\_}. That does not make our data structure incorrect, though. At worst, the producer can report that the queue is full when in fact, it is no longer full. We could read both values atomically, but this will only degrade the performance, and the caller still has to handle the case when the queue is full. Similarly, when \texttt{pop()} reports that the queue is empty, it may no longer be so by the time the call completes. Again, these are the inevitable complexities of concurrency: each operation reflects the state of the data at some point in time. By the time the caller gets the return value and can analyze it, the data may have changed already.

Another note-worthy detail is the careful management of the queue elements' lifetime. We default-construct all elements in the array, so the proper way to transfer the data from the caller into the queue during \texttt{push()} is by copy or move assignment (\texttt{std::forward} does both). On the other hand, once a value is returned to the caller by \texttt{pop()}, we never need that value again, so the right operation here is move, first into the optional and then into the caller's return value object. Note that moving an object is not the same as destroying it; indeed, the moved-from array elements are not destroyed until the queue itself is. If an array element is reused, it is copy- or move-assigned a new value, and assignments are two of the three operations that are safe to do on a moved-from object (the third one is the destructor, which we will also call eventually).

The single-producer single-consumer pattern is a common pattern that allows a programmer to greatly simplify their concurrent data structures. There are others, you can find them in books and papers dedicated to concurrent data structures. All these patterns are ultimately designed to help you write data structures that perform correctly and efficiently when accessed by multiple threads. We, however, must move on and finally tackle the problem of using these threads to get some useful work done.

\section{Concurrent execution patterns}

The next group of patterns for concurrency we must learn are execution patterns. These patterns are used to organize the computations done on multiple threads. You will find out that, just as with the synchronization patterns we saw earlier, all of these are low-level patterns: most solutions for practical problems must combine these patterns into larger, more complex, designs. This is not because C++ is ill-suited for such larger designs; if anything, it is the opposite: there are so many ways to implement, for example, a thread pool, in C++, that for every concrete application, there is a version that is ideally suited in terms of performance and features. This is why it is hard to describe these more complete solutions as patterns: while the problems they address are common, the solutions vary a great deal. But all of these designs have a number of challenges to resolve, and the solutions to those challenges usually use the same tools over and over, so we can at least describe these more basic challenges and their common solutions as design patterns.

\subsection{Active object}

The first concurrent execution pattern we are going to see is the Active Object. An active object usually encapsulates the code to be executed, the data needed for the execution, and the flow of control needed to execute the code asynchronously. This flow of control could be as simple as a separate thread that the object starts and joins. In most cases, we do not start a new thread for every task, so an active object would have some way to run its code on a multi-threaded executor such as a thread pool. From the caller's point of view, an active object is an object that the caller constructs, initializes with the data, then tells the object to execute itself, and the execution happens asynchronously.

The basic active object looks like this:

\begin{code}
// Example 22
class Job {
  ... data ...
  std::thread t_;
  bool done_ {};
  public:
  Job(... args ...) { ... initialize data ... }
  void operator()() {
    t_ = std::thread([this](){ ... computations ... }
  );
  }
  void wait() {
    if (done_) return;
    t_.join();
    done_ = true;
  }
  ~Job() { wait(); }
  auto get() { this->wait(); return ... results ...; }
};
Job j(... args ...);
j();     // Execute code on a thread
... do other work ...
std::cout << j.get();  // Wait for results and print them
\end{code}

In the simplest case shown here, the active object contains a thread that is used to execute the code asynchronously. In most practical cases you would use an executor that schedules the work on one of the threads it manages, but this gets us into implementation-specific details. The execution starts when \texttt{operator()} is called; we can also make the object execute as soon as it is constructed by calling \texttt{operator()} from the constructor. At some point, we have to wait for the results. If we use a separate thread, we can join the thread at that time (and take care to not attempt to join it twice if the caller calls \texttt{wait()} again). If the object represents not a thread but a task in a thread pool or some other executor, we would do the cleanup necessary for that case.

As you can see, once we settle on a particular way to execute code asynchronously, writing active objects with different data and code is a rather repetitive task. Nobody writes active objects the way we just did, we always use some generic reusable framework. There are two general approaches to implementing such a framework. The first one uses inheritance: the base class does the boilerplate work, and the derived class contains the unique task-specific data and code. Staying with our simple approach to active objects, we could write the base class as follows:

\begin{code}
// Example 23
class Job {
  std::thread t_;
  bool done_ {};
  virtual void operator()() = 0;
  public:
  void wait() {
    if (done_) return;
    t_.join();
    done_ = true;
  }
  void run() {
    t_ = std::thread([this](){ (*this)(); });
  }
  virtual ~Job() { wait(); }
};
\end{code}

The base object \texttt{Job} contains everything needed to implement the asynchronous control flow: the thread and the state flag needed to join the thread only once. It also defines the way to execute the code by calling the non-virtual function \texttt{run()}. The code that is executed on the thread must be provided by the derived object by overriding \texttt{operator()}. Note that only \texttt{run()} is public, and \texttt{operator()} is not: this is the non-virtual idiom in action (we saw it in \emph{Chapter 14, The Template Method Pattern and the} \emph{Non-Virtual Idiom}).

The derived object is problem-specific, of course, but generally looks like this:

\begin{code}
// Example 23
class TheJob final : public Job {
  ... data ...
  void operator()() override { ... work ... }
  public:
  TheJob(... args ...) {
    ... initialize data ...
    this->run();
  }
  auto get() { this->wait(); return ... results ...; }
};
\end{code}

The only subtlety here is the call to \texttt{run()} done at the end of the constructor of the derived object. It is not necessary (we can execute the active object later ourselves) but if we do want the constructor to run it, it has to be done in the derived class. If we start the thread and the asynchronous execution in the base class constructor, then we will have a race between the execution on the thread -- \texttt{operator()} -- and the rest of the initialization which continues in the derived class constructor. For the same reason, an active object that starts executing from the constructor should not be derived from again; we ensure that by making the object final.

The use of our active object is very simple: we create it, the object starts executing the code in the background (on a separate thread), and when we need the result we ask for it (this may involve waiting):

\begin{code}
// Example 23
TheJob j1(... args ...);
TheJob j2(... args ...);
... do other stuff ...
std::cout << "Results: " << j1.get() << " " << j2.get();
\end{code}

If you've written any concurrent code in C++ at all, you will have definitely used an active object already: \texttt{std::thread} is an active object, it lets us execute arbitrary code on a separate thread. There are concurrency libraries for C++ where a thread is a base object and all concrete threads are derived from it. But this is not the approach chosen for the C++ standard thread. It follows the second way to implement a reusable active object: type erasure. If you need to familiarize yourself with it, reread \emph{Chapter 6, Understanding Type Erasure}. Even though \texttt{std::thread} itself is a type-erased active object, we're going to implement our own just to demonstrate the design (the standard library code is rather hard to read). This time, there is no base class. The framework is provided by a single class:

\begin{code}
// Example 24
class Job {
  bool done_ {};
  std::function<void()> f_;
  std::thread t_;
  public:
  template <typename F> explicit Job(F&& f) :
    f_(f), t_(f_) {}
  void wait() {
     if (done_) return;
     t_.join();
     done_ = true;
  }
  ~Job() { wait(); }
};
\end{code}

To implement the type-erased callable, we use \texttt{std::function} (we could also use one of the more efficient implementations from \emph{Chapter 6, Understanding Type Erasure}, or implement type erasure ourselves following the same approach). The code supplied by the caller to be executed on a thread comes from the callable \texttt{f} in the constructor argument. Note that the order of the class members is very important: the asynchronous execution starts as soon as the thread \texttt{t\_} is initialized, so other data members, in particular, the callable \texttt{f\_}, must be initialized before that happens.

To use the active object of this style, we need to supply a callable. It could be a lambda expression or a named object, for example:

\begin{code}
// Example 24
class TheJob {
  ... data ...
  public:
  TheJob(... args ...) { ... initialize data ... }
  void operator()() { // Callable!
    ... do the work ...
  }
};
Job j(TheJob(... args ...));
j.wait();
\end{code}

Note that, in this design, there is no easy way to access the data members of the callable \texttt{TheJob}, unless it was created as a named object. For this reason, the results are usually returned through arguments passed to the constructor by reference (the same as we do with \texttt{std::thread}):

\begin{code}
// Example 24
class TheJob {
  ... data ...
  double& res_; // Result
  public:
  TheJob(double& res, ... args ...) : res_(res) {
    ... initialize data ...
  }
  void operator()() { // Callable!
    ... do the work ...
    res_ = ... result ...
  }
};
double res = 0;
Job j(TheJob(res, ... args ...));
j.wait();
std::cout << res;
\end{code}

Active objects can be found in every concurrent C++ program, but some uses of them are common and specialized, and so are recognized as concurrent design patterns in their own right. We will now see several of these patterns.

\subsection{Reactor Object pattern}

The Reactor pattern often uses for event handling or responding to service requests. It solves a specific problem where we have multiple requests for certain actions that are issued by multiple threads; however, the nature of these actions is such that at least part of them must be executed on one thread or otherwise synchronized. The reactor object is the object that services these requests: it accepts requests from multiple threads and executes them.

Here is an example of a reactor that can accept requests to perform a specific computation with caller-supplied inputs and store the results. The requests can come from any number of threads. Each request is allocated a slot in the array of results -- that is the part that must be synchronized across all threads. After the slot is allocated, we can do the computations concurrently. To implement this reactor, we are going to use the atomic index to allocate unique array slots to each request:

\begin{code}
// Example 25
class Reactor {
  static constexpr size_t N = 1024;
  Data data_[N] {};
  std::atomic<size_t> size_{0};
  public:
  bool operator()(... args ...) {
    const size_t s =
      size_.fetch_add(1, std::memory_order_acq_rel);
    if (s >= N) return false;  // Array is full
    data_[s] = ... result ...;
    return true;
  }
  void print_results() { ... }
};
\end{code}

The calls to \texttt{operator()} are thread-safe: any number of threads can call this operator simultaneously, and each call will add the result of the computation to the next array slot without overwriting any data produced by other calls. To retrieve the results from the object, we can either wait until all requests are done or implement another synchronization mechanism such as the publishing protocol to make calls to \texttt{operator()} and \texttt{print\_results()} thread-safe with respect to each other.

Note that usually, a reactor object processes requests asynchronously: it has a separate thread to execute the computations and a queue to channel all the requests to a single thread. We can build such a reactor by combining several patterns we saw earlier, for example, we can add a thread-safe queue to our basic reactor to get an asynchronous reactor (we are about to see an example of such a design).

So far, we focused on starting and executing jobs, and then we wait for the work to complete. The next pattern focuses on handling the completion of asynchronous tasks.

\subsection{Proactor Object pattern}

The Proactor pattern is used to execute asynchronous tasks, usually long-running, by requests from one or more threads. This sounds a lot like the Reactor, but the difference is what happens when a task is done: in the case of the Reactor, we just have to wait for the work to get done (the wait can be blocking or non-blocking, but in all cases, the caller initiates the check for completion). The Proactor object associates each task with a callback, and the callback is executed asynchronously when the task is done. The Reactor and the Proactor are the synchronous and asynchronous solutions to the same problem: handling the completion of concurrent tasks.

A proactor object typically has a queue of tasks to be executed asynchronously or uses another executor to schedule these tasks. Each task is submitted with a callback, usually a callable. The callback is executed when the task is done; often, the same thread that executed the task will also invoke the callback. Since the callback is always asynchronous, care must be taken if it needs to modify any shared data (for example, any data that is accessed by the thread that submitted the task to the proactor). If there is any data shared between the callback and other threads, it must be accessed in a thread-safe way.

Here is an example of a proactor object that uses the thread-safe queue from the previous section. In this example, each task takes one integer as input and computes a \texttt{double} result:

\begin{code}
// Example 26
class Proactor {
  using callback_t = std::function<void(size_t, double)>;
  struct op_task {
    size_t n;
    callback_t f;
  };
  std::atomic<bool> done_{false}; // Must come before t_
  ts_queue<op_task> q_;           // Must come before t_
  std::thread t_;
  public:
  Proactor() : t_([this]() {
    while (true) {
      auto task = q_.pop();
      if (!task) {                // Queue is empty
        if (done_.load(std::memory_order_relaxed)) {
          return;                 // Work is done
        }
        continue;                 // Wait for more work
      }
      ... do the work ...
      double x = ... result ...
      task->f(n, x);
    } // while (true)
  }) {}
  template <typename F>
  void operator()(size_t n, F&& f) {
    q_.push(op_task{n, std::forward<F>(f)});
  }
  ~Proactor() {
    done_.store(true, std::memory_order_relaxed);
    t_.join();
  }
};
\end{code}

The queue stores the work requests which consist of the input and the callable; any number of threads can call \texttt{operator()} to add requests to the queue. A more generic proactor might take a callable for the work request instead of having the computation coded into the concrete proactor object. The proactor executes all the requests in order on a single thread. When the requested computation is done, the thread invokes the callback and passes the result to it. This is how we may use such a proactor object:

\begin{code}
// Example 26
Proactor p;
for (size_t n : ... all inputs ...) {
  p(n, [](double x) { std::cout << x << std::endl; });
}
\end{code}

Note that our proactor executes all callbacks on one thread, and the main thread does not do any output. Otherwise, we would have to protect \texttt{std::cout} with a mutex.

The Proactor pattern is used to both execute asynchronous events and perform additional actions (callbacks) when these events happen. The last pattern we explore in this section does not execute anything but is used to react to external events.

\subsection{Monitor pattern}

The Monitor pattern is used when we need to observe, or monitor, some conditions and respond to certain events. Usually, a monitor runs on its own thread that is sleeping or waiting most of the time. The thread is awakened either by a notification or simply by the passage of time. Once awakened, the monitor object examines the state of the system it is tasked to observe. It may take certain actions if the specified conditions are met, then the thread goes back to waiting.

We are going to see a monitor implementation that uses a timeout; a monitor with a condition variable can be implemented using the same approach but with the waiting on notification pattern as seen earlier in this chapter.

First, we need something to monitor. Let us say that we have several producer threads that do some computations and store results in an array using an atomic index:

\begin{code}
// Example 27
static constexpr size_t N = 1UL << 16;
struct Data {... data ... };
Data data[N] {};
std::atomic<size_t> index(0);
void produce(std::atomic<size_t>& count) {
  for (size_t n = 0; ; ++n) {
    const size_t s =
      index.fetch_add(1, std::memory_order_acq_rel);
    if (s >= N) return;
    const int niter = 1 << (8 + data[s].n);
    data[s] = ... result ...
    count.store(n + 1, std::memory_order_relaxed);
  }
}
\end{code}

Our producer also stores the count of results computed by the thread in the \texttt{count} variable passed to it. Here is how we launch the producer threads:

\begin{code}
// Example 27
std::thread t[nthread];
std::atomic<size_t> work_count[nthread] = {};
for (size_t i = 0; i != nthread; ++i) {
  t[i] = std::thread(produce, std::ref(work_count[i]));
}
\end{code}

We have one result count per thread, so each producer has its own count to increment. Why, then, did we make the counts atomic? Because the counts are also what we are going to monitor: our monitor thread will periodically report on how much work is done. Thus, each work count is accessed by two threads, the producer and the monitor, and we need to either use atomic operations or a mutex to avoid data races.

The monitor is going to be a separate thread that wakes up every now and then, reads the values of the result counts, and reports the progress of the work:

\begin{code}
// Example 27
std::atomic<bool> done {false};
std::thread monitor([&]() {
  auto print = [&]() { ... print work_count[] ... };
  std::cout << "work counts:" << std::endl;
  while (!done.load(std::memory_order_relaxed)) {
    std::this_thread::sleep_for(
      std::chrono::duration<double, std::milli>(500));
    print();
  }
  print();
});
\end{code}

The monitor can be started before the producer threads, or at any time we need to monitor the progress of the work, and it will report how many results are computed by each producer thread, for example:

\begin{code}
work counts:
1096 1083 957 1046 1116 -> 5298/65536
2286 2332 2135 2242 2335 -> 11330/65536
...
13153 13061 13154 12979 13189 -> 65536/65536
13153 13061 13154 12979 13189 -> 65536/65536
\end{code}

Here we used five threads to compute the total of 64K results, and the monitor reports the counts for each thread and the total result count. To shut down the monitor, we need to set the \texttt{done} flag and join the monitor thread:

\begin{code}
// Example 27
done.store(true, std::memory_order_relaxed);
monitor.join();
\end{code}

The other common variant of the Monitor pattern is the one where, instead of waiting on a timer, we wait on a condition. This monitor is a combination of the basic monitor and the pattern for waiting on a notification that we saw earlier in this chapter.

The concurrent programming community has come up with many other patterns for solving common problems related to concurrency; most of these can be used in C++ programs but they are not specific to C++. There are C++-specific features such as atomic variables that influence the way we implement and use these patterns. The examples from this chapter should give you enough guidance to be able to adapt any other concurrent pattern to C++.

The description of execution patterns mostly concludes the brief study of C++ patterns for concurrency. Before you turn the last page, I want to show you a totally different type of concurrent pattern that is just coming to C++.

\section{Coroutine patterns in C++}

Coroutines are a very recent addition to C++: they were introduced in C++20, and their present state is a foundation for building libraries and frameworks as opposed to features you should use in the application code directly. It is a complex feature with many subtle details, and it would take an entire chapter to explain what it does (there is a chapter like that in my book \emph{The Art of Writing Efficient Programs}). Briefly, coroutines are functions that can suspend and resume themselves. They cannot be forced to suspend, a coroutine continues to execute until it suspends itself. They are used to implement what is known as cooperative multitasking, where multiple streams of execution voluntarily yield control to each other rather than being forcibly preempted by the OS.

Every execution pattern we saw in this chapter, and many more, can be implemented using coroutines. It is, however, too early to say whether this is going to become a common use of coroutines in C++, so we cannot say whether a ``proactor coroutine'' will ever become a pattern. One application of coroutines, however, is well on the way to becoming a new pattern in C++: the coroutine generator.

This pattern comes into play when we want to take some computation that is normally done with a complex loop and rewrite it as an iterator. For example, let us say that we have a 3D array and we want to iterate over all its elements and do some computation on them. This is easy enough to do with a loop:

\begin{code}
size_t*** a; // 3D array
for (size_t i = 0; i < N1; ++i) {
  for (size_t j = 0; j < N2; ++j) {
    for (size_t k = 0; k < N3; ++k) {
      ... do work with a[i][j][k] ...
    }
  }
}
\end{code}

But it's hard to write reusable code this way: if we need to customize the work we do on each array element, we have to modify the inner loop. It would be much easier if we had an iterator that runs over the entire 3D array. Unfortunately, to implement this iterator we have to turn the loops inside out: first, we increment \texttt{k} until it reaches \texttt{N3}; then, we increment \texttt{j} by one and go back to incrementing \texttt{k}, and so on. The result is a very convoluted code that reduced many a programmer to counting on their fingers to avoid one-off errors:

\begin{code}
// Example 28
class Iterator {
  const size_t N1, N2, N3;
  size_t*** const a;
  size_t i = 0, j = 0, k = 0;
  bool done = false;
  public:
  Iterator(size_t*** a, size_t N1, size_t N2, size_t N3) :
    N1(N1), N2(N2), N3(N3), a(a) {}
  bool next(size_t& x) {
    if (done) return false;
    x = a[i][j][k];
    if (++k == N3) {
      k = 0;
      if (++j == N2) {
        j = 0;
        if (++i == N1) return (done = true);
      }
    }
    return true;
  }
};
\end{code}

We even took a shortcut and gave our iterator a non-standard interface:

\begin{code}
// Example 28
Iterator it(a, N1, N2, N3);
size_t val;
while (it.next(val)) {
  ... val is the current array element ...
}
\end{code}

The implementation is even more convoluted if we want to conform to the STL iterator interface.

Problems such as this, where a complex function such as our nested loop must be suspended in the middle of the execution so the caller can execute some arbitrary code and resume the suspended function, are an ideal fit for coroutines. Indeed, a coroutine that produces the same sequence as our iterator looks very simple and natural:

\begin{code}
// Example 28
generator<size_t>
coro(size_t*** a, size_t N1, size_t N2, size_t N3) {
  for (size_t i = 0; i < N1; ++i) {
    for (size_t j = 0; j < N2; ++j) {
      for (size_t k = 0; k < N3; ++k) {
        co_yield a[i][j][k];
      }
    }
  }
}
\end{code}

That's it; we have a function that takes the parameters necessary to loop over a 3D array, a regular nested loop, and we do something to each element. The secret is in that innermost line where ``something'' happens: the C++20 keyword \texttt{co\_yield} suspends the coroutine and returns the value \texttt{a{[}i{]}{[}j{]}{[}k{]}} to the caller. It is very similar to the \texttt{return} operator, except \texttt{co\_yield} does not exit the coroutine permanently: the caller can resume the coroutine, and the execution continues from the next line after \texttt{co\_yield}.

The use of this coroutine is also straightforward:

\begin{code}
// Example 28
auto gen = coro(a, N1, N2, N3);
while (true) {
  const size_t val = gen();
  if (!gen) break;
  ... val is the current array element ...
}
\end{code}

The coroutine magic happens inside the generator object that is returned by the coroutine. Its implementation is anything but simple, and, if you want to write one yourself, you have to become an expert on C++ coroutines (and do so by reading another book or article). You can find a very minimal implementation in \emph{Example 28}, and, with the help of a good reference for coroutines, you can understand its inner working line by line. Fortunately, if all you want is to write code like that shown previously, you don't really have to learn the details of the coroutines: there are several open-source libraries that provide utility types such as the generator (with slightly different interfaces), and in C++23 \texttt{std::generator} will be added to the standard library.

While it is certainly easier to write the coroutine with a loop and \texttt{co\_yield} than the convoluted inverted loop of the iterator, what is the price of this convenience? Obviously, you have to either write a generator or find one in a library, but once that is done, are there any more disadvantages to the coroutines? In general, a coroutine involves more work than a regular function, but the performance of the resulting code depends greatly on the compiler and can vary with seemingly insignificant changes to the code (as is the case for any compiler optimizations). The coroutines are still quite new and the compilers do not have comprehensive optimizations for them. That being said, the performance of coroutines can be comparable to that of the hand-crafted iterator. For our \emph{Example 28}, the current (at the moment of this writing) release of Clang 17 gives the following results:

\begin{code}
Iterator time: 9.20286e-10 s/iteration
Generator time: 6.39555e-10 s/iteration
\end{code}

On the other hand, GCC 13 gives an advantage to the iterator:

\begin{code}
Iterator time: 6.46543e-10 s/iteration
Generator time: 1.99748e-09 s/iteration
\end{code}

We can expect the compilers to get better at optimizing coroutines in the future.

Another variant of the coroutine generator is useful when the sequence of values that we want to produce is not limited in advance and we want to generate new elements only when they are needed (lazy generator). Again, the advantage of coroutines is the simplicity of returning results to the caller from inside of the loop.

Here is a simple random number generator implemented as a coroutine:

\begin{code}
// Example 29
generator<size_t> coro(size_t i) {
  while (true) {
    constexpr size_t m = 1234567890, k = 987654321;
    for (size_t j = 0; j != 11; ++j) {
      if (1) i = (i + k) % m; else ++i;
    }
    co_yield i;
  }
}
\end{code}

This coroutine never ends: it suspends itself to return the next pseudo-random number \texttt{i}, and every time it is resumed, the execution jumps back into the infinite loop. Again, the generator is a rather complex object with a lot of boilerplate code that you would be better off getting from a library (or waiting until C++23). But once that's done, the use of the generator is very simple:

\begin{code}
// Example 29
auto gen = coro(42);
size_t random_number = gen();
\end{code}

Every time you call \texttt{gen()}, you get a new random number (of rather poor quality since we have implemented one of the oldest and simplest pseudo-random number generators, so consider this example useful for illustration only). The generator can be called as many times as you need; when it is finally destroyed, so is the coroutine.

We will likely see more design patterns that take advantage of coroutines develop in the coming years. For now, the generator is the only established one, and just recently at that, so it is fitting to conclude the last chapter of the book on C++ design patterns with the newest addition to our pattern toolbox.

\section{Summary}

In this chapter, we explored common C++ solutions to the problems of developing concurrent software. This is a very different type of problem compared to everything we studied before. Our main concerns here are correctness, specifically, by avoiding data races, and performance. Synchronization patterns are standard ways to control access to shared data to avoid undefined behavior. Execution patterns are the basic building blocks of thread schedulers and asynchronous executors. Finally, the high-level patterns and guidelines for the concurrent design are the ways we, the programmers, keep our sanity while trying to think about all the things that could happen before, after, or at the same time as one another.

\section{Questions}

\begin{enumerate}
\item
  What is concurrency?
\item
  How does C++ support concurrency?
\item
  What are synchronization design patterns?
\item
  What are execution design patterns?
\item
  What overall guidelines for the design and the architecture of concurrent programs should be followed?
\item
  What is a transactional interface?
\end{enumerate}

